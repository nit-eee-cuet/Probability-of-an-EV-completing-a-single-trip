% import data from given csv file
EVData = readtable("ElectricCarData_Clean.csv"); 	
EVData.RapidCharge = categorical(EVData.RapidCharge);
EVData.PowerTrain = categorical(EVData.PowerTrain);
EVData.PlugType = categorical(EVData.PlugType);
EVData.BodyStyle = categorical(EVData.BodyStyle);

% pre-processing of imported data
EVData.kWh_per_100km = EVData.Efficiency_Wh_per_km/10;
EVData = removevars(EVData,"Efficiency_Wh_per_km");
EVData.Efficiency_km_per_kwh = (EVData.kWh_per_100km/100).^(-1);
EVData.FastCharge_kW = EVData.FastCharge_kmH./EVData.Efficiency_km_per_kwh;
EVData = removevars(EVData,"FastCharge_kmH");

% assigning to a new table
EVData1 = EVData; 	

% adding new features 
12	for i = 1:height(EVData1) 
13	     if EVData1.FastCharge_kW(i)>=0 && EVData1.FastCharge_kW(i)<=25
14	         EVData1.FastCharge_kW(i) = 22;
15	         continue
16	     elseif EVData1.FastCharge_kW(i)>25 && EVData1.FastCharge_kW(i)<40
17	         EVData1.FastCharge_kW(i) = 36;
18	         continue
19	     elseif EVData1.FastCharge_kW(i)>=40 && EVData1.FastCharge_kW(i)<=60
20	         EVData1.FastCharge_kW(i) = 50;
21	         continue
22	     elseif EVData1.FastCharge_kW(i)>60 && EVData1.FastCharge_kW(i)<=90
23	         EVData1.FastCharge_kW(i) = 75;
24	         continue
25	     elseif EVData1.FastCharge_kW(i)>90 && EVData1.FastCharge_kW(i)<=135
26	         EVData1.FastCharge_kW(i) = 120;
27	         continue
28	     elseif EVData1.FastCharge_kW(i)>135
29	         EVData1.FastCharge_kW(i) = 150;
30	         continue
31	     end
32	end    

33	EVData1.FastCharge_kW = categorical(EVData1.FastCharge_kW);
34	EVData1.EVBatteryCapacity_kWh = (EVData1.Range_km/100).*EVData1.kWh_per_100km;

% single trip status of an EV to cover Dhaka-Chattogram highway
35	for i = 1:height(EVData1) 
36	     if EVData1.EVBatteryCapacity_kWh(i)>=EVData1.kWh_per_100km(i)
37	         EVData1.SingleTrip_Status(i) = "Covered";
38	         continue
39	     elseif EVData1.EVBatteryCapacity_kWh(i)<EVData1.kWh_per_100km(i)
40	         EVData1.SingleTrip_Status(i) = "Not Covered";
41	         continue
42	     end
43	end
44	EVData1.SingleTrip_Status = categorical(EVData1.SingleTrip_Status);

% Classification model which can define the probability of covered single trip status of an EV

45	function [trainedClassifier, validationAccuracy] = trainClassifier(trainingData)
46	% [trainedClassifier, validationAccuracy] = trainClassifier(trainingData)
47	% returns a trained classifier and its accuracy. This code recreates the
48	% classification model trained in Classification Learner app. It is to be used the
49	% generated code to automate training the same model with new data, or to
50	% learn how to programmatically train models.
51	%
52	%  Input:
53	%      trainingData: A table containing the same predictor and response
54	%       columns as those imported into the app.
55	%
56	%  Output:
57	%      trainedClassifier: A struct containing the trained classifier. The
58	%       struct contains various fields with information about the trained
59	%       classifier.
60	%
61	%      trainedClassifier.predictFcn: A function to make predictions on new
62	%       data.
63	%
64	%      validationAccuracy: A double containing the accuracy in percent. In
65	%       the app, the History list displays this overall accuracy score for
66	%       each model.
67	%
68	% the code is used to train the model with new data. To retrain classifier,
69	% by calling the function from the command line with original data or new
70	% data as the input argument trainingData.
71	%
72	% For example, to retrain a classifier trained with the original data set
73	% T, enter:
74	%   [trainedClassifier, validationAccuracy] = trainClassifier(T)
75	%
76	% To make predictions with the returned 'trainedClassifier' on new data T2,
77	% use
78	%   yfit = trainedClassifier.predictFcn(T2)
79	%
80	% T2 must be a table containing at least the same predictor columns as used
81	% during training. For details, enter:
82	%   trainedClassifier.HowToPredict

83	% Auto-generated by MATLAB on 06-Jun-2022 08:25:58

84	% Extract predictors and response
85	% This code processes the data into the right shape for training the
86	% model

87	inputTable = trainingData;
88	predictorNames = {'Range_km', 'kWh_per_100km', 'Efficiency_km_per_kwh', 'FastCharge_kW', 'EVBatteryCapacity_kWh'};
89	predictors = inputTable(:, predictorNames);
90	response = inputTable.SingleTrip_Status;
91	isCategoricalPredictor = [false, false, false, true, false];
92	 
93	% Train a classifier
94	% This code specifies all the classifier options and trains the classifier.
95	% For logistic regression, the response values must be converted to zeros
96	% and ones because the responses are assumed to follow a binomial
97	% distribution.
98	% 1 or true = 'successful' class
99	% 0 or false = 'failure' class
100	% NaN - missing response

101	successClass = 'Covered';
102	failureClass = 'Not Covered';
103	% Compute the majority response class. If there is a NaN-prediction from
104	% fitglm, convert NaN to this majority class label.
105	numSuccess = sum(response == successClass);
106	numFailure = sum(response == failureClass);
107	if numSuccess > numFailure
108	    missingClass = successClass;
109	else
110	    missingClass = failureClass;
111	end
112	responseCategories = {successClass, failureClass};
113	successFailureAndMissingClasses = categorical({successClass; failureClass; missingClass}, responseCategories);
114	isMissing = isundefined(response);
115	zeroOneResponse = double(ismember(response, successClass));
116	zeroOneResponse(isMissing) = NaN;
117	% Prepare input arguments to fitglm.
118	concatenatedPredictorsAndResponse = [predictors, table(zeroOneResponse)];
119	% Train using fitglm.
120	GeneralizedLinearModel = fitglm(...
121	    concatenatedPredictorsAndResponse, ...
122	    'Distribution', 'binomial', ...
123	    'link', 'logit');
 
124	% Convert predicted probabilities to predicted class labels and scores.
125	convertSuccessProbsToPredictions = @(p) successFailureAndMissingClasses( ~isnan(p).*( (p<0.5) + 1 ) + isnan(p)*3 );
126	returnMultipleValuesFcn = @(varargin) varargin{1:max(1,nargout)};
127	scoresFcn = @(p) [p, 1-p];
128	predictionsAndScoresFcn = @(p) returnMultipleValuesFcn( convertSuccessProbsToPredictions(p), scoresFcn(p) );

129	% Create the result struct with predict function
130	predictorExtractionFcn = @(t) t(:, predictorNames);
131	logisticRegressionPredictFcn = @(x) predictionsAndScoresFcn( predict(GeneralizedLinearModel, x) );
132	trainedClassifier.predictFcn = @(x) logisticRegressionPredictFcn(predictorExtractionFcn(x));

133	% Add additional fields to the result struct
134	trainedClassifier.RequiredVariables = {'EVBatteryCapacity_kWh', 'Efficiency_km_per_kwh', 'FastCharge_kW', 'Range_km', 'kWh_per_100km'};
135	trainedClassifier.GeneralizedLinearModel = GeneralizedLinearModel;
136	trainedClassifier.SuccessClass = successClass;
137	trainedClassifier.FailureClass = failureClass;
138	trainedClassifier.MissingClass = missingClass;
139	trainedClassifier.ClassNames = {successClass; failureClass};
140	trainedClassifier.About = 'This struct is a trained model exported from Classification Learner R2020a.';
141	trainedClassifier.HowToPredict = sprintf('To make predictions on a new table, T, use: \n  yfit = c.predictFcn(T) \nreplacing ''c'' with the name of the variable that is this struct, e.g. ''trainedModel''. \n \nThe table, T, must contain the variables returned by: \n  c.RequiredVariables \nVariable formats (e.g. matrix/vector, datatype) must match the original training data. \nAdditional variables are ignored. \n \nFor more information, see <a href="matlab:helpview(fullfile(docroot, ''stats'', ''stats.map''), ''appclassification_exportmodeltoworkspace'')">How to predict using an exported model</a>.');
 
142	% Extract predictors and response
143	% This code processes the data into the right shape for training the
144	% model.
145	inputTable = trainingData;
146	predictorNames = {'Range_km', 'kWh_per_100km', 'Efficiency_km_per_kwh', 'FastCharge_kW', 'EVBatteryCapacity_kWh'};
147	predictors = inputTable(:, predictorNames);
148	response = inputTable.SingleTrip_Status;
149	isCategoricalPredictor = [false, false, false, true, false];
 
150	% Perform cross-validation
151	KFolds = 5;
152	cvp = cvpartition(response, 'KFold', KFolds);
153	% Initialize the predictions to the proper sizes
154	validationPredictions = response;
155	numObservations = size(predictors, 1);
156	numClasses = 2;
157	validationScores = NaN(numObservations, numClasses);
158	for fold = 1:KFolds
159	    trainingPredictors = predictors(cvp.training(fold), :);
160	    trainingResponse = response(cvp.training(fold), :);
161	    foldIsCategoricalPredictor = isCategoricalPredictor;
162	    
163	    % Train a classifier
164	    % This code specifies all the classifier options and trains the classifier.
165	    % For logistic regression, the response values must be converted to zeros
166	    % and ones because the responses are assumed to follow a binomial
167	    % distribution.
168	    % 1 or true = 'successful' class
169	    % 0 or false = 'failure' class
170	    % NaN - missing response.
171	    successClass = 'Covered';
172	    failureClass = 'Not Covered';
173	    % Compute the majority response class. If there is a NaN-prediction from
174	    % fitglm, convert NaN to this majority class label.
175	    numSuccess = sum(trainingResponse == successClass);
176	    numFailure = sum(trainingResponse == failureClass);
177	    if numSuccess > numFailure
178	        missingClass = successClass;
179	    else
180	        missingClass = failureClass;
181	    end
182	    responseCategories = {successClass, failureClass};
183	    successFailureAndMissingClasses = categorical({successClass; failureClass; missingClass}, responseCategories);
184	    isMissing = isundefined(trainingResponse);
185	    zeroOneResponse = double(ismember(trainingResponse, successClass));
186	    zeroOneResponse(isMissing) = NaN;
187	    % Prepare input arguments to fitglm.
188	    concatenatedPredictorsAndResponse = [trainingPredictors, table(zeroOneResponse)];
189	    % Train using fitglm.
190	    GeneralizedLinearModel = fitglm(...
191	        concatenatedPredictorsAndResponse, ...
192	        'Distribution', 'binomial', ...
193	        'link', 'logit');

194	    % Convert predicted probabilities to predicted class labels and scores.
195	    convertSuccessProbsToPredictions = @(p) successFailureAndMissingClasses( ~isnan(p).*( (p<0.5) + 1 ) + isnan(p)*3 );
196	    returnMultipleValuesFcn = @(varargin) varargin{1:max(1,nargout)};
197	    scoresFcn = @(p) [p, 1-p];
198	    predictionsAndScoresFcn = @(p) returnMultipleValuesFcn( convertSuccessProbsToPredictions(p), scoresFcn(p) );
199	    
200	    % Create the result struct with predict function
201	    logisticRegressionPredictFcn = @(x) predictionsAndScoresFcn( predict(GeneralizedLinearModel, x) );
202	    validationPredictFcn = @(x) logisticRegressionPredictFcn(x);

203	    % Add additional fields to the result struct
204	    % Compute validation predictions
205	    validationPredictors = predictors(cvp.test(fold), :);
206	    [foldPredictions, foldScores] = validationPredictFcn(validationPredictors);

207	    % Store predictions in the original order
208	    validationPredictions(cvp.test(fold), :) = foldPredictions;
209	    validationScores(cvp.test(fold), :) = foldScores;
210	end

211	% Compute validation accuracy
212	correctPredictions = (validationPredictions == response);
213	isMissing = ismissing(response);
214	correctPredictions = correctPredictions(~isMissing);
validationAccuracy = sum(correctPredictions)/length(correctPredictions);
